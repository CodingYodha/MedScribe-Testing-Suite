{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee923bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.__init__ = lambda *a, **k: __import__('tqdm').tqdm(*a, **{**k, \"disable\": False})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ad5202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c5cf730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (large-v3 on GPU)...\n",
      "Whisper loaded in 6.06 sec\n",
      "[After Whisper Load] GPU Allocated: 0.91 GB, Reserved: 1.66 GB\n",
      "\n",
      "Loading Gemma 1B (4-bit quantized on GPU)...\n",
      "Gemma loaded in 4.10 sec\n",
      "[After Gemma Load] GPU Allocated: 1.24 GB, Reserved: 2.25 GB\n",
      "\n",
      "================================================================================\n",
      "PROCESSING PRESCRIPTION\n",
      "================================================================================\n",
      "\n",
      "1. Transcribing English pass...\n",
      "   ✓ English done in 12.71 sec\n",
      "\n",
      "2. Transcribing Marathi pass...\n",
      "   ✓ Marathi done in 27.81 sec\n",
      "\n",
      "3. Extracting medical entities...\n",
      "   ✓ Extraction done in 60.85 sec\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "Model Load Time    : 10.16 sec\n",
      "English Pass Time  : 12.71 sec\n",
      "Marathi Pass Time  : 27.81 sec\n",
      "Entity Extraction  : 60.85 sec\n",
      "TOTAL PIPELINE     : 101.37 sec\n",
      "================================================================================\n",
      "[Final] GPU Allocated: 0.91 GB, Reserved: 2.25 GB\n",
      "\n",
      "ENGLISH TRANSCRIPT:\n",
      " Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high.  But don't worry. It's the early stage.  You take Metformin 500mg in the morning and evening after eating one tablet.  And take 2-3 spoons of Live 1252 syrup every day.  Stop eating oily and sugary foods.  Take a walk every day for 30 minutes.  One more thing, do an ultrasound of your abdomen for the next visit.  I want to see your liver condition because of what's going on with it.  And take medicine continuously for 30 days and follow up.  And yes, don't eat late at night.  Otherwise, you won't have sugar control.  Okay? Let's see.\n",
      "\n",
      "MARATHI TRANSCRIPT:\n",
      " मिस्टर पाटेल, तुमचे रिपोर्ट्स पाइले आणि मला दिसता कि तुम्हाला थोड़े फैटी लिवर आणि शुगर लेवल आई हाई, पण टेंशन गिव नका, इस दी अरले स्टेज, तुम्ही मेटाफॉर्मिन 500 mg सकाली आणि रातरी जेवना अंतर एक टाबलेट गिय  वन मो थिंग, नेक्स्ट विजिट ला अल्ट्रा सॉंड ऑफ अपडोमन करुण आना, तुमचे लिवर चे कंट्रीशन मला बगाय चाहे काई सीन है तच मनुन, आणि 30 दिवस तुम्ही मेडिसन कंटिनिवसली घ्या अंतरला फॉल्व अप करा, आणि हो जेवन वेल वर घ्\n",
      "\n",
      "EXTRACTED ENTITIES:\n",
      "ERROR: {\"medicines\": [{\"name\": \"Metformin\", \"dosage\": \"500mg\", \"frequency\": \"morning and evening\", \"duration\": \"30 days\"}], \"diseases\": [\"fatty liver\", \"sugar levels\"], \"tests\": [{\"name\": \"ultasonogram\", \"timing\": \"next visit\"}, \"description\": \"your abdomen\"]}\n",
      "\n",
      "✓ Results saved to prescription_output.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNVIDIA RTX 3050 (4GB VRAM) or better:\\n- Model Load: ~5-8 sec (one-time)\\n- English Pass: ~2-3 sec\\n- Marathi Pass: ~3-4 sec\\n- Entity Extraction: ~3-5 sec\\n- TOTAL: ~10-15 sec ✅ ACCEPTABLE\\n\\nVRAM Usage:\\n- Whisper large-v3 (FP16): 3.8 GB\\n- Gemma 1B (4-bit): 0.4 GB\\n- Total: ~4.2 GB\\n\\nIf VRAM insufficient, switch to Whisper Medium:\\nwhisper_model = WhisperModel(\"medium\", device=\"cuda\", compute_type=\"float16\")\\n- VRAM: 1.5 GB (saves 2.3 GB!)\\n- Accuracy: ~5-10% worse\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ============================================\n",
    "# GPU Memory Optimization\n",
    "# ============================================\n",
    "def print_gpu_usage(label=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"[{label}] GPU Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# Load Whisper (GPU-optimized)\n",
    "# ============================================\n",
    "print(\"Loading Whisper model (large-v3 on GPU)...\")\n",
    "start = time.time()\n",
    "model_dir = \"./models/large-v3\" \n",
    "whisper_model = WhisperModel(\n",
    "    model_dir,\n",
    "    device=\"cuda\",\n",
    "    compute_type=\"float16\",  # FP16 for speed\n",
    "    download_root=\"./models/whisper\"\n",
    ")\n",
    "\n",
    "load_time = time.time() - start\n",
    "print(f\"Whisper loaded in {load_time:.2f} sec\")\n",
    "print_gpu_usage(\"After Whisper Load\")\n",
    "\n",
    "# ============================================\n",
    "# Load Gemma (4-bit quantized on GPU)\n",
    "# ============================================\n",
    "print(\"\\nLoading Gemma 1B (4-bit quantized on GPU)...\")\n",
    "start = time.time()\n",
    "\n",
    "model_dir = \"./models/gemma-3-1b-it\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"cuda:0\",  # FORCE GPU!\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "gemma_load_time = time.time() - start\n",
    "print(f\"Gemma loaded in {gemma_load_time:.2f} sec\")\n",
    "print_gpu_usage(\"After Gemma Load\")\n",
    "\n",
    "# ============================================\n",
    "# Transcription Function\n",
    "# ============================================\n",
    "def transcribe_audio(audio_path, language=\"en\"):\n",
    "    \"\"\"Transcribe audio with timing\"\"\"\n",
    "    start = time.time()\n",
    "    segments, info = whisper_model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        beam_size=5,\n",
    "        vad_filter=True  # Voice activity detection for better accuracy\n",
    "    )\n",
    "    \n",
    "    transcript = \" \".join([segment.text for segment in segments])\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return transcript, elapsed\n",
    "\n",
    "# ============================================\n",
    "# Entity Extraction Function\n",
    "# ============================================\n",
    "def extract_entities(transcript):\n",
    "    \"\"\"Extract medical entities using Gemma\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a medical prescription parser. Extract ONLY explicitly stated information.\n",
    "\n",
    "Rules:\n",
    "1. Extract medicines with EXACT dosages mentioned\n",
    "2. If dosage/frequency unclear, mark as \"unspecified\"\n",
    "3. Do NOT infer or assume any information\n",
    "4. Output ONLY valid JSON, no markdown formatting\n",
    "\n",
    "Output format:\n",
    "{\"medicines\": [{\"name\": str, \"dosage\": str, \"frequency\": str, \"duration\": str}], \"diseases\": [str], \"tests\": [{\"name\": str, \"timing\": str}]}\"\"\"\n",
    "\n",
    "    user_prompt = f\"{system_prompt}\\n\\nExtract from:\\n{transcript}\\n\\nJSON output:\"\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(gemma_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = gemma_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.1,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    # Clean output (remove markdown code fences)\n",
    "    result_text = result_text.split(\"JSON output:\")[-1].strip()\n",
    "    result_text = re.sub(r'```json\\s*|\\s*```', '', result_text).strip()\n",
    "    \n",
    "    # Parse JSON\n",
    "    try:\n",
    "        entities = json.loads(result_text)\n",
    "        return entities, elapsed, None\n",
    "    except json.JSONDecodeError as e:\n",
    "        return None, elapsed, result_text\n",
    "\n",
    "# ============================================\n",
    "# Full Pipeline\n",
    "# ============================================\n",
    "def process_prescription(audio_path):\n",
    "    \"\"\"Complete pipeline with timing\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING PRESCRIPTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    # English transcription\n",
    "    print(\"\\n1. Transcribing English pass...\")\n",
    "    english_transcript, english_time = transcribe_audio(audio_path, language=\"en\")\n",
    "    print(f\"   ✓ English done in {english_time:.2f} sec\")\n",
    "    \n",
    "    # Marathi transcription\n",
    "    print(\"\\n2. Transcribing Marathi pass...\")\n",
    "    marathi_transcript, marathi_time = transcribe_audio(audio_path, language=\"hi\")  # 'hi' for Hindi/Marathi\n",
    "    print(f\"   ✓ Marathi done in {marathi_time:.2f} sec\")\n",
    "    \n",
    "    # Entity extraction (use English transcript)\n",
    "    print(\"\\n3. Extracting medical entities...\")\n",
    "    entities, extraction_time, error = extract_entities(english_transcript)\n",
    "    print(f\"   ✓ Extraction done in {extraction_time:.2f} sec\")\n",
    "    \n",
    "    total_time = time.time() - pipeline_start\n",
    "    \n",
    "    # ============================================\n",
    "    # SUMMARY\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model Load Time    : {load_time + gemma_load_time:.2f} sec\")\n",
    "    print(f\"English Pass Time  : {english_time:.2f} sec\")\n",
    "    print(f\"Marathi Pass Time  : {marathi_time:.2f} sec\")\n",
    "    print(f\"Entity Extraction  : {extraction_time:.2f} sec\")\n",
    "    print(f\"TOTAL PIPELINE     : {total_time:.2f} sec\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print_gpu_usage(\"Final\")\n",
    "    \n",
    "    # Results\n",
    "    print(\"\\nENGLISH TRANSCRIPT:\")\n",
    "    print(english_transcript)\n",
    "    print(\"\\nMARATHI TRANSCRIPT:\")\n",
    "    print(marathi_transcript)\n",
    "    print(\"\\nEXTRACTED ENTITIES:\")\n",
    "    if entities:\n",
    "        print(json.dumps(entities, indent=2))\n",
    "    else:\n",
    "        print(f\"ERROR: {error}\")\n",
    "    \n",
    "    return {\n",
    "        \"timings\": {\n",
    "            \"model_load\": load_time + gemma_load_time,\n",
    "            \"english\": english_time,\n",
    "            \"marathi\": marathi_time,\n",
    "            \"extraction\": extraction_time,\n",
    "            \"total\": total_time\n",
    "        },\n",
    "        \"transcripts\": {\n",
    "            \"english\": english_transcript,\n",
    "            \"marathi\": marathi_transcript\n",
    "        },\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# Example Usage\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Test with your audio file\n",
    "    audio_file = r\"E:\\Projects\\Med_Scribe\\Testing\\Mr_Patil_Medical_converstaino.m4a\"\n",
    "    \n",
    "    result = process_prescription(audio_file)\n",
    "    \n",
    "    # Save results\n",
    "    with open(\"prescription_output.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(\"\\n✓ Results saved to prescription_output.json\")\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# EXPECTED PERFORMANCE (GPU)\n",
    "# ============================================\n",
    "\"\"\"\n",
    "NVIDIA RTX 3050 (4GB VRAM) or better:\n",
    "- Model Load: ~5-8 sec (one-time)\n",
    "- English Pass: ~2-3 sec\n",
    "- Marathi Pass: ~3-4 sec\n",
    "- Entity Extraction: ~3-5 sec\n",
    "- TOTAL: ~10-15 sec ✅ ACCEPTABLE\n",
    "\n",
    "VRAM Usage:\n",
    "- Whisper large-v3 (FP16): 3.8 GB\n",
    "- Gemma 1B (4-bit): 0.4 GB\n",
    "- Total: ~4.2 GB\n",
    "\n",
    "If VRAM insufficient, switch to Whisper Medium:\n",
    "whisper_model = WhisperModel(\"medium\", device=\"cuda\", compute_type=\"float16\")\n",
    "- VRAM: 1.5 GB (saves 2.3 GB!)\n",
    "- Accuracy: ~5-10% worse\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c61626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (large-v3)...\n",
      "Whisper loaded in 5.61 sec\n",
      "[After Whisper Load] GPU Allocated: 0.91 GB, Reserved: 3.43 GB\n",
      "\n",
      "Loading Gemma 1B (4-bit quantized)...\n",
      "Gemma loaded in 7.81 sec\n",
      "[After Gemma Load] GPU Allocated: 1.24 GB, Reserved: 3.43 GB\n",
      "\n",
      "================================================================================\n",
      "PROCESSING PRESCRIPTION\n",
      "================================================================================\n",
      "\n",
      "1. Transcribing English pass...\n",
      "   ✓ English done in 30.28 sec\n",
      "\n",
      "2. Transcribing Marathi pass...\n",
      "\n",
      "3. Extracting medical entities...\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "Model Load Time    : 13.42 sec\n",
      "English Pass Time  : 30.28 sec\n",
      "TOTAL PIPELINE     : 30.28 sec\n",
      "================================================================================\n",
      "[Final] GPU Allocated: 0.91 GB, Reserved: 3.43 GB\n",
      "\n",
      "ENGLISH TRANSCRIPT:\n",
      "Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high. But don't worry. It's the early stage. You take Metformin 500mg in the morning and evening after eating one tablet. And take 2-3 spoons of Live 1252 syrup every day. Stop eating oily and sugary foods. Take a walk every day for 30 minutes. One more thing, do an ultrasound of your abdomen for the next visit. I want to see your liver condition because of what's going on with it. And take medicine continuously for 30 days and follow up. And yes, don't eat late at night. Otherwise, you won't have sugar control. Okay? Let's see.\n",
      "\n",
      "MARATHI TRANSCRIPT:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'marathi_transcript' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 203\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    202\u001b[39m     audio_file = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mE:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mProjects\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMed_Scribe\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mTesting\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mMr_Patil_Medical_converstaino.m4a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     result = \u001b[43mprocess_prescription\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    205\u001b[39m     \u001b[38;5;66;03m# Save output\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mprescription_output.json\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mprocess_prescription\u001b[39m\u001b[34m(audio_path)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mprint\u001b[39m(english_transcript)\n\u001b[32m    178\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMARATHI TRANSCRIPT:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmarathi_transcript\u001b[49m)\n\u001b[32m    180\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEXTRACTED ENTITIES:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(entities, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'marathi_transcript' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# ============================================\n",
    "# GPU UTILITIES\n",
    "# ============================================\n",
    "def print_gpu_usage(label=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"[{label}] GPU Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD WHISPER\n",
    "# ============================================\n",
    "print(\"Loading Whisper model (large-v3)...\")\n",
    "start = time.time()\n",
    "whisper_model = WhisperModel(\n",
    "    \"./models/large-v3\",\n",
    "    device=\"cuda\",\n",
    "    compute_type=\"float16\",\n",
    "    download_root=\"./models/whisper\"\n",
    ")\n",
    "load_time_whisper = time.time() - start\n",
    "print(f\"Whisper loaded in {load_time_whisper:.2f} sec\")\n",
    "print_gpu_usage(\"After Whisper Load\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD GEMMA (4-BIT QUANTIZED)\n",
    "# ============================================\n",
    "print(\"\\nLoading Gemma 1B (4-bit quantized)...\")\n",
    "start = time.time()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_dir = \"./models/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "load_time_gemma = time.time() - start\n",
    "print(f\"Gemma loaded in {load_time_gemma:.2f} sec\")\n",
    "print_gpu_usage(\"After Gemma Load\")\n",
    "\n",
    "# ============================================\n",
    "# TRANSCRIPTION (CHUNKED)\n",
    "# ============================================\n",
    "def transcribe_audio(audio_path, language=\"en\"):\n",
    "    \"\"\"Chunked transcription with Whisper\"\"\"\n",
    "    start = time.time()\n",
    "    segments, _ = whisper_model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        beam_size=5,\n",
    "        vad_filter=True\n",
    "    )\n",
    "    # Combine segments for chunked processing\n",
    "    transcript_chunks = [segment.text.strip() for segment in segments if segment.text.strip()]\n",
    "    elapsed = time.time() - start\n",
    "    return transcript_chunks, elapsed\n",
    "\n",
    "# ============================================\n",
    "# STREAMING GEMMA ENTITY EXTRACTION\n",
    "# ============================================\n",
    "def extract_entities(transcript_chunks):\n",
    "    \"\"\"Chunk-wise entity extraction with strict JSON output\"\"\"\n",
    "    system_prompt = \"\"\"You are a medical prescription parser. Extract ONLY explicitly stated information.\n",
    "\n",
    "Rules:\n",
    "1. Extract medicines with EXACT dosages mentioned\n",
    "2. If dosage/frequency unclear, mark as \"unspecified\"\n",
    "3. Do NOT infer or assume any information\n",
    "4. Output ONLY valid JSON with the following format:\n",
    "{\n",
    "  \"medicines\": [{\"name\": str, \"dosage\": str, \"frequency\": str, \"duration\": str}],\n",
    "  \"diseases\": [str],\n",
    "  \"tests\": [{\"name\": str, \"timing\": str}]\n",
    "}\"\"\"\n",
    "\n",
    "    final_entities = {\n",
    "        \"medicines\": [],\n",
    "        \"diseases\": [],\n",
    "        \"tests\": []\n",
    "    }\n",
    "\n",
    "    total_time = 0\n",
    "    for chunk in transcript_chunks:\n",
    "        user_prompt = f\"{system_prompt}\\n\\nExtract from:\\n{chunk}\\n\\nJSON output:\"\n",
    "\n",
    "        inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(gemma_model.device)\n",
    "\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = gemma_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,  # limit per chunk\n",
    "                do_sample=False,\n",
    "                temperature=0.1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        elapsed = time.time() - start\n",
    "        total_time += elapsed\n",
    "\n",
    "        result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        result_text = result_text.split(\"JSON output:\")[-1].strip()\n",
    "        result_text = re.sub(r'```json\\s*|\\s*```', '', result_text).strip()\n",
    "\n",
    "        try:\n",
    "            entities = json.loads(result_text)\n",
    "            # Merge results\n",
    "            final_entities[\"medicines\"].extend(entities.get(\"medicines\", []))\n",
    "            final_entities[\"diseases\"].extend([d for d in entities.get(\"diseases\", []) if d not in final_entities[\"diseases\"]])\n",
    "            final_entities[\"tests\"].extend(entities.get(\"tests\", []))\n",
    "        except json.JSONDecodeError:\n",
    "            # Skip invalid chunk, optionally log\n",
    "            print(\"⚠️ JSON decode failed for chunk, skipping\")\n",
    "\n",
    "    return final_entities, total_time\n",
    "\n",
    "# ============================================\n",
    "# FULL PIPELINE\n",
    "# ============================================\n",
    "def process_prescription(audio_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING PRESCRIPTION\")\n",
    "    print(\"=\"*80)\n",
    "    pipeline_start = time.time()\n",
    "\n",
    "    # English transcription\n",
    "    print(\"\\n1. Transcribing English pass...\")\n",
    "    english_chunks, english_time = transcribe_audio(audio_path, language=\"en\")\n",
    "    english_transcript = \" \".join(english_chunks)\n",
    "    print(f\"   ✓ English done in {english_time:.2f} sec\")\n",
    "\n",
    "    # Marathi transcription (optional, skip if not required)\n",
    "    print(\"\\n2. Transcribing Marathi pass...\")\n",
    "    # marathi_chunks, marathi_time = transcribe_audio(audio_path, language=\"hi\")\n",
    "    # marathi_transcript = \" \".join(marathi_chunks)\n",
    "    # print(f\"   ✓ Marathi done in {marathi_time:.2f} sec\")\n",
    "\n",
    "    # Entity extraction (chunked)\n",
    "    print(\"\\n3. Extracting medical entities...\")\n",
    "    # entities, extraction_time = extract_entities(english_chunks)\n",
    "    # print(f\"   ✓ Extraction done in {extraction_time:.2f} sec\")\n",
    "\n",
    "    total_time = time.time() - pipeline_start\n",
    "\n",
    "    # ============================================\n",
    "    # SUMMARY\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model Load Time    : {load_time_whisper + load_time_gemma:.2f} sec\")\n",
    "    print(f\"English Pass Time  : {english_time:.2f} sec\")\n",
    "    # print(f\"Marathi Pass Time  : {marathi_time:.2f} sec\")\n",
    "    # print(f\"Entity Extraction  : {extraction_time:.2f} sec\")\n",
    "    print(f\"TOTAL PIPELINE     : {total_time:.2f} sec\")\n",
    "    print(\"=\"*80)\n",
    "    print_gpu_usage(\"Final\")\n",
    "\n",
    "    # Results\n",
    "    print(\"\\nENGLISH TRANSCRIPT:\")\n",
    "    print(english_transcript)\n",
    "    print(\"\\nMARATHI TRANSCRIPT:\")\n",
    "    print(marathi_transcript)\n",
    "    print(\"\\nEXTRACTED ENTITIES:\")\n",
    "    print(json.dumps(entities, indent=2))\n",
    "\n",
    "    return {\n",
    "        \"timings\": {\n",
    "            \"model_load\": load_time_whisper + load_time_gemma,\n",
    "            \"english\": english_time,\n",
    "            # \"marathi\": marathi_time,\n",
    "            # \"extraction\": extraction_time,\n",
    "            \"total\": total_time\n",
    "        },\n",
    "        \"transcripts\": {\n",
    "            \"english\": english_transcript,\n",
    "            \"marathi\": marathi_transcript\n",
    "        },\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# MAIN\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"E:\\Projects\\Med_Scribe\\Testing\\Mr_Patil_Medical_converstaino.m4a\"\n",
    "    result = process_prescription(audio_file)\n",
    "\n",
    "    # Save output\n",
    "    with open(\"prescription_output.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "\n",
    "    print(\"\\n✓ Results saved to prescription_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09175fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper model (large-v3)...\n",
      "Whisper loaded in 7.06 sec\n",
      "[After Whisper Load] GPU Allocated: 0.91 GB, Reserved: 3.43 GB\n",
      "\n",
      "Loading Gemma 1B (4-bit quantized)...\n",
      "Gemma loaded in 4.43 sec\n",
      "[After Gemma Load] GPU Allocated: 1.24 GB, Reserved: 3.43 GB\n",
      "\n",
      "================================================================================\n",
      "PROCESSING PRESCRIPTION\n",
      "================================================================================\n",
      "\n",
      "1. Transcribing English pass...\n",
      "   ✓ English done in 18.05 sec (12 chunks)\n",
      "\n",
      "2. Transcribing Marathi pass...\n",
      "   ✓ Marathi done in 54.94 sec (2 chunks)\n",
      "\n",
      "3. Extracting medical entities...\n",
      "   ✓ Chunk 1/12 processed\n",
      "   ⚠ Chunk 2 failed after 2 retries, skipping\n",
      "   ⚠ Chunk 3 failed after 2 retries, skipping\n",
      "   ⚠ Chunk 4 failed after 2 retries, skipping\n",
      "\n",
      "❌ Error: 'str' object has no attribute 'get'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_36212\\335684099.py\", line 277, in <module>\n",
      "    result = process_prescription(audio_file)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_36212\\335684099.py\", line 228, in process_prescription\n",
      "    entities, extraction_time = extract_entities(english_chunks, max_retries=2)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_36212\\335684099.py\", line 183, in extract_entities\n",
      "    merge_entities(final_entities, entities)\n",
      "  File \"C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_36212\\335684099.py\", line 106, in merge_entities\n",
      "    if med.get(\"name\", \"\").lower() not in existing_med_names:\n",
      "       ^^^^^^^\n",
      "AttributeError: 'str' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from faster_whisper import WhisperModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# ============================================\n",
    "# GPU UTILITIES\n",
    "# ============================================\n",
    "def print_gpu_usage(label=\"\"):\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"[{label}] GPU Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD WHISPER\n",
    "# ============================================\n",
    "print(\"Loading Whisper model (large-v3)...\")\n",
    "start = time.time()\n",
    "whisper_model = WhisperModel(\n",
    "    \"./models/large-v3\",\n",
    "    device=\"cuda\",\n",
    "    compute_type=\"float16\",\n",
    "    download_root=\"./models/whisper\"\n",
    ")\n",
    "load_time_whisper = time.time() - start\n",
    "print(f\"Whisper loaded in {load_time_whisper:.2f} sec\")\n",
    "print_gpu_usage(\"After Whisper Load\")\n",
    "\n",
    "# ============================================\n",
    "# LOAD GEMMA (4-BIT QUANTIZED)\n",
    "# ============================================\n",
    "print(\"\\nLoading Gemma 1B (4-bit quantized)...\")\n",
    "start = time.time()\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_dir = \"./models/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "load_time_gemma = time.time() - start\n",
    "print(f\"Gemma loaded in {load_time_gemma:.2f} sec\")\n",
    "print_gpu_usage(\"After Gemma Load\")\n",
    "\n",
    "# ============================================\n",
    "# TRANSCRIPTION (CHUNKED)\n",
    "# ============================================\n",
    "def transcribe_audio(audio_path, language=\"en\"):\n",
    "    \"\"\"Chunked transcription with Whisper\"\"\"\n",
    "    start = time.time()\n",
    "    segments, _ = whisper_model.transcribe(\n",
    "        audio_path,\n",
    "        language=language,\n",
    "        beam_size=5,\n",
    "        vad_filter=True\n",
    "    )\n",
    "    # Combine segments for chunked processing\n",
    "    transcript_chunks = [segment.text.strip() for segment in segments if segment.text.strip()]\n",
    "    elapsed = time.time() - start\n",
    "    return transcript_chunks, elapsed\n",
    "\n",
    "# ============================================\n",
    "# JSON EXTRACTION UTILITIES\n",
    "# ============================================\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract JSON from text with multiple fallback strategies\"\"\"\n",
    "    # Strategy 1: Find JSON between braces\n",
    "    json_match = re.search(r'\\{[\\s\\S]*\\}', text)\n",
    "    if json_match:\n",
    "        try:\n",
    "            return json.loads(json_match.group())\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "    \n",
    "    # Strategy 2: Try to find and fix common issues\n",
    "    try:\n",
    "        # Remove markdown code blocks\n",
    "        cleaned = re.sub(r'```(?:json)?\\s*|\\s*```', '', text)\n",
    "        # Remove text before first { and after last }\n",
    "        cleaned = re.search(r'\\{[\\s\\S]*\\}', cleaned)\n",
    "        if cleaned:\n",
    "            return json.loads(cleaned.group())\n",
    "    except (json.JSONDecodeError, AttributeError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 3: Return empty structure\n",
    "    return None\n",
    "\n",
    "def merge_entities(target, source):\n",
    "    \"\"\"Merge entity dictionaries without duplicates\"\"\"\n",
    "    # Merge medicines (check for duplicate names)\n",
    "    existing_med_names = {m.get(\"name\", \"\").lower() for m in target[\"medicines\"]}\n",
    "    for med in source.get(\"medicines\", []):\n",
    "        if med.get(\"name\", \"\").lower() not in existing_med_names:\n",
    "            target[\"medicines\"].append(med)\n",
    "            existing_med_names.add(med.get(\"name\", \"\").lower())\n",
    "    \n",
    "    # Merge diseases (unique only)\n",
    "    for disease in source.get(\"diseases\", []):\n",
    "        if disease and disease not in target[\"diseases\"]:\n",
    "            target[\"diseases\"].append(disease)\n",
    "    \n",
    "    # Merge tests (check for duplicate names)\n",
    "    existing_test_names = {t.get(\"name\", \"\").lower() for t in target[\"tests\"]}\n",
    "    for test in source.get(\"tests\", []):\n",
    "        if test.get(\"name\", \"\").lower() not in existing_test_names:\n",
    "            target[\"tests\"].append(test)\n",
    "            existing_test_names.add(test.get(\"name\", \"\").lower())\n",
    "\n",
    "# ============================================\n",
    "# STREAMING GEMMA ENTITY EXTRACTION\n",
    "# ============================================\n",
    "def extract_entities(transcript_chunks, max_retries=2):\n",
    "    \"\"\"Chunk-wise entity extraction with robust JSON parsing\"\"\"\n",
    "    \n",
    "    # Optimized: Create prompt template once\n",
    "    system_prompt = \"\"\"Extract medical information in JSON format:\n",
    "{\"medicines\":[{\"name\":\"\",\"dosage\":\"\",\"frequency\":\"\",\"duration\":\"\"}],\"diseases\":[],\"tests\":[{\"name\":\"\",\"timing\":\"\"}]}\n",
    "\n",
    "Rules: Only extract explicitly stated info. Mark unclear info as \"unspecified\".\"\"\"\n",
    "\n",
    "    final_entities = {\n",
    "        \"medicines\": [],\n",
    "        \"diseases\": [],\n",
    "        \"tests\": []\n",
    "    }\n",
    "\n",
    "    total_time = 0\n",
    "    failed_chunks = 0\n",
    "    \n",
    "    for idx, chunk in enumerate(transcript_chunks):\n",
    "        if not chunk or len(chunk.strip()) < 10:  # Skip very short chunks\n",
    "            continue\n",
    "            \n",
    "        # Optimized: Shorter, clearer prompt\n",
    "        user_prompt = f\"{system_prompt}\\n\\nText: {chunk}\\n\\nJSON:\"\n",
    "\n",
    "        inputs = tokenizer(user_prompt, return_tensors=\"pt\").to(gemma_model.device)\n",
    "\n",
    "        retry_count = 0\n",
    "        chunk_success = False\n",
    "        \n",
    "        while retry_count < max_retries and not chunk_success:\n",
    "            start = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = gemma_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=(retry_count > 0),  # First try deterministic, then sample\n",
    "                    temperature=0.3 if retry_count > 0 else 0.1,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            elapsed = time.time() - start\n",
    "            total_time += elapsed\n",
    "\n",
    "            result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only the JSON part (after \"JSON:\")\n",
    "            if \"JSON:\" in result_text:\n",
    "                result_text = result_text.split(\"JSON:\")[-1].strip()\n",
    "            \n",
    "            # Try to extract and parse JSON\n",
    "            entities = extract_json_from_text(result_text)\n",
    "            \n",
    "            if entities:\n",
    "                # Validate structure\n",
    "                if isinstance(entities, dict):\n",
    "                    merge_entities(final_entities, entities)\n",
    "                    chunk_success = True\n",
    "                    print(f\"   ✓ Chunk {idx+1}/{len(transcript_chunks)} processed\")\n",
    "                else:\n",
    "                    retry_count += 1\n",
    "            else:\n",
    "                retry_count += 1\n",
    "            \n",
    "            if not chunk_success and retry_count >= max_retries:\n",
    "                failed_chunks += 1\n",
    "                print(f\"   ⚠ Chunk {idx+1} failed after {max_retries} retries, skipping\")\n",
    "                break\n",
    "        \n",
    "        # Clear GPU cache periodically\n",
    "        if idx % 5 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    if failed_chunks > 0:\n",
    "        print(f\"\\n⚠️ Warning: {failed_chunks}/{len(transcript_chunks)} chunks failed to parse\")\n",
    "\n",
    "    return final_entities, total_time\n",
    "\n",
    "# ============================================\n",
    "# FULL PIPELINE\n",
    "# ============================================\n",
    "def process_prescription(audio_path):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROCESSING PRESCRIPTION\")\n",
    "    print(\"=\"*80)\n",
    "    pipeline_start = time.time()\n",
    "\n",
    "    # English transcription\n",
    "    print(\"\\n1. Transcribing English pass...\")\n",
    "    english_chunks, english_time = transcribe_audio(audio_path, language=\"en\")\n",
    "    english_transcript = \" \".join(english_chunks)\n",
    "    print(f\"   ✓ English done in {english_time:.2f} sec ({len(english_chunks)} chunks)\")\n",
    "\n",
    "    # Marathi transcription (optional, skip if not required)\n",
    "    print(\"\\n2. Transcribing Marathi pass...\")\n",
    "    marathi_chunks, marathi_time = transcribe_audio(audio_path, language=\"hi\")\n",
    "    marathi_transcript = \" \".join(marathi_chunks)\n",
    "    print(f\"   ✓ Marathi done in {marathi_time:.2f} sec ({len(marathi_chunks)} chunks)\")\n",
    "\n",
    "    # Entity extraction (chunked)\n",
    "    print(\"\\n3. Extracting medical entities...\")\n",
    "    entities, extraction_time = extract_entities(english_chunks, max_retries=2)\n",
    "    print(f\"   ✓ Extraction done in {extraction_time:.2f} sec\")\n",
    "\n",
    "    total_time = time.time() - pipeline_start\n",
    "\n",
    "    # ============================================\n",
    "    # SUMMARY\n",
    "    # ============================================\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model Load Time    : {load_time_whisper + load_time_gemma:.2f} sec\")\n",
    "    print(f\"English Pass Time  : {english_time:.2f} sec\")\n",
    "    print(f\"Marathi Pass Time  : {marathi_time:.2f} sec\")\n",
    "    print(f\"Entity Extraction  : {extraction_time:.2f} sec\")\n",
    "    print(f\"TOTAL PIPELINE     : {total_time:.2f} sec\")\n",
    "    print(\"=\"*80)\n",
    "    print_gpu_usage(\"Final\")\n",
    "\n",
    "    # Results\n",
    "    print(\"\\nENGLISH TRANSCRIPT:\")\n",
    "    print(english_transcript)\n",
    "    print(\"\\nMARATHI TRANSCRIPT:\")\n",
    "    print(marathi_transcript)\n",
    "    print(\"\\nEXTRACTED ENTITIES:\")\n",
    "    print(json.dumps(entities, indent=2, ensure_ascii=False))\n",
    "\n",
    "    return {\n",
    "        \"timings\": {\n",
    "            \"model_load\": load_time_whisper + load_time_gemma,\n",
    "            \"english\": english_time,\n",
    "            \"marathi\": marathi_time,\n",
    "            \"extraction\": extraction_time,\n",
    "            \"total\": total_time\n",
    "        },\n",
    "        \"transcripts\": {\n",
    "            \"english\": english_transcript,\n",
    "            \"marathi\": marathi_transcript\n",
    "        },\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# ============================================\n",
    "# MAIN\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = r\"E:\\Projects\\Med_Scribe\\Testing\\Mr_Patil_Medical_converstaino.m4a\"\n",
    "    \n",
    "    try:\n",
    "        result = process_prescription(audio_file)\n",
    "\n",
    "        # Save output\n",
    "        with open(\"prescription_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(\"\\n✓ Results saved to prescription_output.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medscribe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
