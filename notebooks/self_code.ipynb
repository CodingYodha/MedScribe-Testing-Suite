{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35bcf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiva\\miniconda3\\envs\\medd\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from faster_whisper import WhisperModel,BatchedInferencePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70f3afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper model loaded\n",
      "COnverted to batched\n"
     ]
    }
   ],
   "source": [
    "whisper_model = WhisperModel(\n",
    "     \"./models/large-v3\",\n",
    "     device = \"cuda\",\n",
    "     compute_type=\"float16\",\n",
    "     download_root=\"./models/whisper\"\n",
    ")\n",
    "print(\"Whisper model loaded\")\n",
    "batched_model = BatchedInferencePipeline(model=whisper_model)\n",
    "print(\"COnverted to batched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "914497f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = batched_model.transcribe(    \n",
    "    language='en',\n",
    "    audio= r'E:\\Projects\\Med_Scribe\\Testing\\Mr_Patil_Medical_converstaino.m4a',\n",
    "    beam_size=4,\n",
    "    vad_filter=True,\n",
    "    batch_size=8,\n",
    "    word_timestamps=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "919bfdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.23s -> 28.99s]  Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high. But, don't worry. It's the early stage. Take Metformin 500mg in the morning and evening after eating one tablet. And take 2 tsp of live 1252 syrup twice a day. Stop eating oily and sugary foods.\n",
      "[28.99s -> 54.26s]  Walk for 30 minutes. One more thing, do ultrasound of abdomen on next visit. To check your liver condition. Take medicine continuously for 30 days and follow up. And yes, take food on time. Don't eat late at night. Otherwise sugar control won't happen. Okay? Let's see.\n"
     ]
    }
   ],
   "source": [
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10e5da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in segments:\n",
    "    for word in segment.words:\n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (word.start, word.end, word.word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c60790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "segments_list = list(segments)\n",
    "print(segments_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2db27bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma loaded\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    "    \n",
    ")\n",
    "\n",
    "model_dir = \"./models/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(model_dir,\n",
    "                                                   quantization_config=bnb_config,\n",
    "                                                   device_map = \"auto\",\n",
    "                                                   torch_dtype=torch.float16,\n",
    "                                                   low_cpu_mem_usage=True)\n",
    "print(\"Gemma loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de578c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0e9962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object restore_speech_timestamps at 0x000002D95E639120>\n",
      "==========================================================\n",
      "['Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high.', \"But don't worry. It's the early stage.\", 'You take Metformin 500mg in the morning and evening after eating one tablet.', 'And take 2-3 spoons of Live 1252 syrup every day.', 'Stop eating oily and sugary foods.', 'Take a walk every day for 30 minutes.', 'One more thing, do an ultrasound of your abdomen for the next visit.', \"I want to see your liver condition and what's going on with it.\", 'And take medicine continuously for 30 days and follow up.', \"And yes, don't eat late at night.\", \"Otherwise, you won't have sugar control.\", \"Okay? Let's see.\"]\n",
      "==========================================================\n",
      "Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high. But don't worry. It's the early stage. You take Metformin 500mg in the morning and evening after eating one tablet. And take 2-3 spoons of Live 1252 syrup every day. Stop eating oily and sugary foods. Take a walk every day for 30 minutes. One more thing, do an ultrasound of your abdomen for the next visit. I want to see your liver condition and what's going on with it. And take medicine continuously for 30 days and follow up. And yes, don't eat late at night. Otherwise, you won't have sugar control. Okay? Let's see.\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "segments,_ = whisper_model.transcribe(\n",
    "    language='en',\n",
    "    audio= r'E:\\Projects\\Med_Scribe\\Testing\\Mr_Patil_Medical_converstaino.m4a',\n",
    "    beam_size=4,\n",
    "    vad_filter=True,\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "print(segments)\n",
    "transcript_chunks = [segment.text.strip() for segment in segments if segment.text.strip()]\n",
    "print(58*\"=\")\n",
    "print(transcript_chunks)\n",
    "full_transcript = \" \".join(transcript_chunks)\n",
    "print(58*\"=\")\n",
    "print(full_transcript)\n",
    "print(58*\"=\")\n",
    "segments_list = list(segments)\n",
    "\n",
    "for segment in segments:\n",
    "    print(f\"Start: {segment['start']}, End: {segment['end']}, Text: {segment['text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e712e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_list = list(segments)\n",
    "\n",
    "# Now you can access each segment's data\n",
    "for segment in segments_list:\n",
    "    print(segment)\n",
    "segments_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9914c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnBackticks(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_sequence=\"AAA\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_sequence = stop_sequence\n",
    "        self.stop_ids = tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Check if the last tokens match the stop sequence\n",
    "        if len(input_ids[0]) >= len(self.stop_ids):\n",
    "            if (input_ids[0][-len(self.stop_ids):] == torch.tensor(self.stop_ids, device=input_ids.device)).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnBackticks(tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1335b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "You are a medical prescription parser. Extract ONLY information explicitly stated.\n",
      "\n",
      "Rules:\n",
      "1. Extract medicines with EXACT dosages mentioned\n",
      "2. If dosage/frequency unclear, mark as \"unspecified\"\n",
      "3. Do NOT infer or assume any information\n",
      "4. If doctor says \"continue previous meds\", extract NOTHING\n",
      "5. Output only one valid JSON object and stop\n",
      "6. At the end of the Output print AAA\n",
      "\n",
      "\n",
      "Output format:\n",
      "{\n",
      "  \"medicines\": [{\"name\": str, \"dosage\": str, \"frequency\": str, \"duration\": str}],\n",
      "  \"diseases\": [str],\n",
      "  \"tests\": [{\"name\": str, \"timing\": str}]\n",
      "}\n",
      "\n",
      "Extract from this prescription conversation:\n",
      "Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high. But don't worry. It's the early stage. You take Metformin 500mg in the morning and evening after eating one tablet. And take 2-3 spoons of Live 1252 syrup every day. Stop eating oily and sugary foods. Take a walk every day for 30 minutes. One more thing, do an ultrasound of your abdomen for the next visit. I want to see your liver condition and what's going on with it. And take medicine continuously for 30 days and follow up. And yes, don't eat late at night. Otherwise, you won't have sugar control. Okay? Let's see.\n",
      "\n",
      "Remember: Only extract explicitly stated information. No assumptions.\n",
      "The prescription is:\n",
      "\"Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high. But don't worry. It's the early stage. You take Metformin 500mg in the morning and evening after eating one tablet. And take 2-3 spoons of Live 1252 syrup every day. Stop eating oily and sugary foods. Take a walk every day for 30 minutes. One more thing, do an ultrasound of your abdomen for the next visit. I want to see your liver condition and what's going on with it. And take medicine continuously for 30 days and follow up. And yes, don't eat late at night. Otherwise, you won't have sugar control.\"\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"medicines\": [\n",
      "    {\n",
      "      \"name\": \"Metformin\",\n",
      "      \"dosage\": \"500mg\",\n",
      "      \"frequency\": \"morning and evening\",\n",
      "      \"duration\": \"30 days\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Live 1252\",\n",
      "      \"dosage\": \"2-3 spoons\",\n",
      "      \"frequency\": \"every day\"\n",
      "    }\n",
      "  ],\n",
      "  \"diseases\": [\n",
      "    \"fatty liver\",\n",
      "    \"sugar levels\"\n",
      "  ],\n",
      "  \"tests\": [\n",
      "    \"ultrasound of your abdomen\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "AAA\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a medical prescription parser. Extract ONLY information explicitly stated.\n",
    "\n",
    "Rules:\n",
    "1. Extract medicines with EXACT dosages mentioned\n",
    "2. If dosage/frequency unclear, mark as \"unspecified\"\n",
    "3. Do NOT infer or assume any information\n",
    "4. If doctor says \"continue previous meds\", extract NOTHING\n",
    "5. Output only one valid JSON object and stop\n",
    "6. At the end of the Output print AAA\n",
    "\n",
    "\n",
    "Output format:\n",
    "{\n",
    "  \"medicines\": [{\"name\": str, \"dosage\": str, \"frequency\": str, \"duration\": str}],\n",
    "  \"diseases\": [str],\n",
    "  \"tests\": [{\"name\": str, \"timing\": str}]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "final_entities = {\n",
    "    \"medicines\" : [],\n",
    "    \"diseases\" : [],\n",
    "    \"tests\" : []\n",
    "}\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "{system_prompt}\n",
    "Extract from this prescription conversation:\n",
    "{full_transcript}\n",
    "\n",
    "Remember: Only extract explicitly stated information. No assumptions.\n",
    "\"\"\"\n",
    "# user_prompt = f\"{system_prompt}\\n\\n Text:{full_transcript}\"\n",
    "inputs = tokenizer(user_prompt,return_tensors = 'pt' ).to(gemma_model.device)\n",
    "outputs = gemma_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.01,\n",
    "    pad_token_id = tokenizer.pad_token_id,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    do_sample=False\n",
    "\n",
    ")\n",
    "result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab371ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soundfile']\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_66156\\3761534279.py:3: UserWarning: torchaudio._backend.get_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  print(torchaudio.get_audio_backend())\n",
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_66156\\3761534279.py:6: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "print(torchaudio.list_audio_backends())\n",
    "print(torchaudio.get_audio_backend())\n",
    "\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc0bc19",
   "metadata": {},
   "source": [
    "loads the diarization part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c8d6251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Local\\Temp\\ipykernel_529268\\2500100860.py:6: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2642944]) 48000\n",
      "Embeddings shape: torch.Size([1, 1, 192])\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "\n",
    "signal, fs = torchaudio.load(r\"E:\\Projects\\Med_Scribe\\Testing\\audio.wav\")\n",
    "print(signal.shape, fs)\n",
    "\n",
    "# Load local ECAPA model\n",
    "model_dir = \"pretrained_models/spkrec-ecapa-voxceleb\"\n",
    "classifier = EncoderClassifier.from_hparams(source=model_dir, savedir=None, run_opts={\"device\":\"cuda\"})\n",
    "\n",
    "# Load audio\n",
    "signal, fs = torchaudio.load(r\"E:\\Projects\\Med_Scribe\\Testing\\audio.wav\")\n",
    "\n",
    "# Get speaker embeddings\n",
    "embeddings = classifier.encode_batch(signal)  # shape: (1, embedding_size, frames)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "# You can now cluster embeddings for speaker diarization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29027ea9",
   "metadata": {},
   "source": [
    "diarization part working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cbb3df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00s - 0.29s: Speaker 1\n",
      "0.29s - 0.57s: Speaker 1\n",
      "0.57s - 0.86s: Speaker 0\n",
      "0.86s - 1.15s: Speaker 0\n",
      "1.15s - 1.43s: Speaker 1\n",
      "1.43s - 1.72s: Speaker 1\n",
      "1.72s - 2.01s: Speaker 1\n",
      "2.01s - 2.29s: Speaker 0\n",
      "2.29s - 2.58s: Speaker 0\n",
      "2.58s - 2.87s: Speaker 0\n",
      "2.87s - 3.15s: Speaker 1\n",
      "3.15s - 3.44s: Speaker 1\n",
      "3.44s - 3.73s: Speaker 0\n",
      "3.73s - 4.01s: Speaker 1\n",
      "4.01s - 4.30s: Speaker 1\n",
      "4.30s - 4.59s: Speaker 1\n",
      "4.59s - 4.88s: Speaker 0\n",
      "4.88s - 5.16s: Speaker 1\n",
      "5.16s - 5.45s: Speaker 0\n",
      "5.45s - 5.74s: Speaker 1\n",
      "5.74s - 6.02s: Speaker 1\n",
      "6.02s - 6.31s: Speaker 0\n",
      "6.31s - 6.60s: Speaker 1\n",
      "6.60s - 6.88s: Speaker 1\n",
      "6.88s - 7.17s: Speaker 1\n",
      "7.17s - 7.46s: Speaker 0\n",
      "7.46s - 7.74s: Speaker 0\n",
      "7.74s - 8.03s: Speaker 1\n",
      "8.03s - 8.32s: Speaker 1\n",
      "8.32s - 8.60s: Speaker 0\n",
      "8.60s - 8.89s: Speaker 0\n",
      "8.89s - 9.18s: Speaker 0\n",
      "9.18s - 9.46s: Speaker 0\n",
      "9.46s - 9.75s: Speaker 0\n",
      "9.75s - 10.04s: Speaker 1\n",
      "10.04s - 10.32s: Speaker 1\n",
      "10.32s - 10.61s: Speaker 1\n",
      "10.61s - 10.90s: Speaker 1\n",
      "10.90s - 11.18s: Speaker 0\n",
      "11.18s - 11.47s: Speaker 1\n",
      "11.47s - 11.76s: Speaker 1\n",
      "11.76s - 12.04s: Speaker 0\n",
      "12.04s - 12.33s: Speaker 0\n",
      "12.33s - 12.62s: Speaker 0\n",
      "12.62s - 12.90s: Speaker 1\n",
      "12.90s - 13.19s: Speaker 0\n",
      "13.19s - 13.48s: Speaker 0\n",
      "13.48s - 13.77s: Speaker 0\n",
      "13.77s - 14.05s: Speaker 1\n",
      "14.05s - 14.34s: Speaker 0\n",
      "14.34s - 14.63s: Speaker 1\n",
      "14.63s - 14.91s: Speaker 0\n",
      "14.91s - 15.20s: Speaker 0\n",
      "15.20s - 15.49s: Speaker 1\n",
      "15.49s - 15.77s: Speaker 1\n",
      "15.77s - 16.06s: Speaker 0\n",
      "16.06s - 16.35s: Speaker 0\n",
      "16.35s - 16.63s: Speaker 0\n",
      "16.63s - 16.92s: Speaker 0\n",
      "16.92s - 17.21s: Speaker 0\n",
      "17.21s - 17.49s: Speaker 1\n",
      "17.49s - 17.78s: Speaker 1\n",
      "17.78s - 18.07s: Speaker 0\n",
      "18.07s - 18.35s: Speaker 0\n",
      "18.35s - 18.64s: Speaker 1\n",
      "18.64s - 18.93s: Speaker 0\n",
      "18.93s - 19.21s: Speaker 1\n",
      "19.21s - 19.50s: Speaker 0\n",
      "19.50s - 19.79s: Speaker 0\n",
      "19.79s - 20.07s: Speaker 0\n",
      "20.07s - 20.36s: Speaker 0\n",
      "20.36s - 20.65s: Speaker 1\n",
      "20.65s - 20.93s: Speaker 1\n",
      "20.93s - 21.22s: Speaker 0\n",
      "21.22s - 21.51s: Speaker 0\n",
      "21.51s - 21.80s: Speaker 1\n",
      "21.80s - 22.08s: Speaker 0\n",
      "22.08s - 22.37s: Speaker 1\n",
      "22.37s - 22.66s: Speaker 0\n",
      "22.66s - 22.94s: Speaker 1\n",
      "22.94s - 23.23s: Speaker 1\n",
      "23.23s - 23.52s: Speaker 0\n",
      "23.52s - 23.80s: Speaker 1\n",
      "23.80s - 24.09s: Speaker 0\n",
      "24.09s - 24.38s: Speaker 0\n",
      "24.38s - 24.66s: Speaker 1\n",
      "24.66s - 24.95s: Speaker 0\n",
      "24.95s - 25.24s: Speaker 0\n",
      "25.24s - 25.52s: Speaker 0\n",
      "25.52s - 25.81s: Speaker 1\n",
      "25.81s - 26.10s: Speaker 0\n",
      "26.10s - 26.38s: Speaker 0\n",
      "26.38s - 26.67s: Speaker 0\n",
      "26.67s - 26.96s: Speaker 0\n",
      "26.96s - 27.24s: Speaker 0\n",
      "27.24s - 27.53s: Speaker 0\n",
      "27.53s - 27.82s: Speaker 1\n",
      "27.82s - 28.10s: Speaker 1\n",
      "28.10s - 28.39s: Speaker 1\n",
      "28.39s - 28.68s: Speaker 0\n",
      "28.68s - 28.96s: Speaker 0\n",
      "28.96s - 29.25s: Speaker 0\n",
      "29.25s - 29.54s: Speaker 1\n",
      "29.54s - 29.82s: Speaker 0\n",
      "29.82s - 30.11s: Speaker 1\n",
      "30.11s - 30.40s: Speaker 0\n",
      "30.40s - 30.69s: Speaker 1\n",
      "30.69s - 30.97s: Speaker 1\n",
      "30.97s - 31.26s: Speaker 0\n",
      "31.26s - 31.55s: Speaker 1\n",
      "31.55s - 31.83s: Speaker 0\n",
      "31.83s - 32.12s: Speaker 1\n",
      "32.12s - 32.41s: Speaker 0\n",
      "32.41s - 32.69s: Speaker 0\n",
      "32.69s - 32.98s: Speaker 0\n",
      "32.98s - 33.27s: Speaker 1\n",
      "33.27s - 33.55s: Speaker 1\n",
      "33.55s - 33.84s: Speaker 0\n",
      "33.84s - 34.13s: Speaker 1\n",
      "34.13s - 34.41s: Speaker 0\n",
      "34.41s - 34.70s: Speaker 1\n",
      "34.70s - 34.99s: Speaker 1\n",
      "34.99s - 35.27s: Speaker 1\n",
      "35.27s - 35.56s: Speaker 0\n",
      "35.56s - 35.85s: Speaker 1\n",
      "35.85s - 36.13s: Speaker 0\n",
      "36.13s - 36.42s: Speaker 0\n",
      "36.42s - 36.71s: Speaker 0\n",
      "36.71s - 36.99s: Speaker 0\n",
      "36.99s - 37.28s: Speaker 1\n",
      "37.28s - 37.57s: Speaker 0\n",
      "37.57s - 37.85s: Speaker 0\n",
      "37.85s - 38.14s: Speaker 0\n",
      "38.14s - 38.43s: Speaker 1\n",
      "38.43s - 38.71s: Speaker 1\n",
      "38.71s - 39.00s: Speaker 0\n",
      "39.00s - 39.29s: Speaker 1\n",
      "39.29s - 39.58s: Speaker 1\n",
      "39.58s - 39.86s: Speaker 0\n",
      "39.86s - 40.15s: Speaker 0\n",
      "40.15s - 40.44s: Speaker 1\n",
      "40.44s - 40.72s: Speaker 0\n",
      "40.72s - 41.01s: Speaker 1\n",
      "41.01s - 41.30s: Speaker 0\n",
      "41.30s - 41.58s: Speaker 1\n",
      "41.58s - 41.87s: Speaker 0\n",
      "41.87s - 42.16s: Speaker 1\n",
      "42.16s - 42.44s: Speaker 0\n",
      "42.44s - 42.73s: Speaker 0\n",
      "42.73s - 43.02s: Speaker 0\n",
      "43.02s - 43.30s: Speaker 1\n",
      "43.30s - 43.59s: Speaker 0\n",
      "43.59s - 43.88s: Speaker 0\n",
      "43.88s - 44.16s: Speaker 0\n",
      "44.16s - 44.45s: Speaker 0\n",
      "44.45s - 44.74s: Speaker 0\n",
      "44.74s - 45.02s: Speaker 0\n",
      "45.02s - 45.31s: Speaker 0\n",
      "45.31s - 45.60s: Speaker 0\n",
      "45.60s - 45.88s: Speaker 0\n",
      "45.88s - 46.17s: Speaker 0\n",
      "46.17s - 46.46s: Speaker 1\n",
      "46.46s - 46.74s: Speaker 1\n",
      "46.74s - 47.03s: Speaker 0\n",
      "47.03s - 47.32s: Speaker 0\n",
      "47.32s - 47.61s: Speaker 0\n",
      "47.61s - 47.89s: Speaker 1\n",
      "47.89s - 48.18s: Speaker 0\n",
      "48.18s - 48.47s: Speaker 1\n",
      "48.47s - 48.75s: Speaker 1\n",
      "48.75s - 49.04s: Speaker 1\n",
      "49.04s - 49.33s: Speaker 0\n",
      "49.33s - 49.61s: Speaker 0\n",
      "49.61s - 49.90s: Speaker 0\n",
      "49.90s - 50.19s: Speaker 0\n",
      "50.19s - 50.47s: Speaker 0\n",
      "50.47s - 50.76s: Speaker 1\n",
      "50.76s - 51.05s: Speaker 1\n",
      "51.05s - 51.33s: Speaker 1\n",
      "51.33s - 51.62s: Speaker 1\n",
      "51.62s - 51.91s: Speaker 1\n",
      "51.91s - 52.19s: Speaker 0\n",
      "52.19s - 52.48s: Speaker 0\n",
      "52.48s - 52.77s: Speaker 0\n",
      "52.77s - 53.05s: Speaker 0\n",
      "53.05s - 53.34s: Speaker 0\n",
      "53.34s - 53.63s: Speaker 0\n",
      "53.63s - 53.91s: Speaker 0\n",
      "53.91s - 54.20s: Speaker 1\n",
      "54.20s - 54.49s: Speaker 1\n",
      "54.49s - 54.77s: Speaker 0\n",
      "54.77s - 55.06s: Speaker 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import torchaudio\n",
    "\n",
    "# Load your local model\n",
    "model_dir = \"pretrained_models/spkrec-ecapa-voxceleb\"\n",
    "classifier = EncoderClassifier.from_hparams(source=model_dir, savedir=None, run_opts={\"device\":\"cuda\"})\n",
    "\n",
    "# Load audio\n",
    "signal, fs = torchaudio.load(r\"E:\\Projects\\Med_Scribe\\Testing\\audio.wav\")\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = classifier.encode_batch(signal)  # shape: (1, embedding_size, frames)\n",
    "embeddings = embeddings.squeeze(0).T.cpu().detach().numpy()  # shape: (frames, embedding_size)\n",
    "\n",
    "# Cluster embeddings to diarize\n",
    "num_speakers = 2  # change as needed\n",
    "clustering = AgglomerativeClustering(n_clusters=num_speakers)\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "# Convert labels to timestamps (rough approximation)\n",
    "frame_duration = signal.shape[1] / fs / embeddings.shape[0]\n",
    "for i, label in enumerate(labels):\n",
    "    start_time = i * frame_duration\n",
    "    end_time = (i + 1) * frame_duration\n",
    "    print(f\"{start_time:.2f}s - {end_time:.2f}s: Speaker {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68a3d140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shiva\\miniconda3\\envs\\medd\\Lib\\site-packages\\speechbrain\\utils\\checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "c:\\Users\\Shiva\\miniconda3\\envs\\medd\\Lib\\site-packages\\speechbrain\\processing\\features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n",
      "c:\\Users\\Shiva\\miniconda3\\envs\\medd\\Lib\\site-packages\\speechbrain\\processing\\features.py:1529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected ~5 speakers\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import torchaudio\n",
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Load speaker embedding model\n",
    "model_dir = \"pretrained_models/spkrec-ecapa-voxceleb\"\n",
    "classifier = EncoderClassifier.from_hparams(source=model_dir, savedir=None, run_opts={\"device\":\"cuda\"})\n",
    "\n",
    "# 2️⃣ Load audio\n",
    "audio_path = r\"E:\\Projects\\Med_Scribe\\Testing\\audio.wav\"\n",
    "signal, fs = torchaudio.load(audio_path)\n",
    "\n",
    "# 3️⃣ Compute speaker embeddings\n",
    "window = int(fs * 3.0)   # 3 sec window\n",
    "stride = int(fs * 1.5)   # 50% overlap\n",
    "embeddings = []\n",
    "\n",
    "for start in range(0, signal.shape[1] - window, stride):\n",
    "    chunk = signal[:, start:start + window]\n",
    "    emb = classifier.encode_batch(chunk).squeeze(0).cpu().detach().numpy()\n",
    "    embeddings.append(emb)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "\n",
    "# 4️⃣ Cluster embeddings to assign speaker labels\n",
    "from sklearn.metrics import silhouette_score\n",
    "best_score, best_k = -1, 1\n",
    "for k in range(1, 6):  # try up to 5 speakers\n",
    "    tmp_labels = AgglomerativeClustering(n_clusters=k).fit_predict(embeddings)\n",
    "    if len(set(tmp_labels)) > 1:\n",
    "        score = silhouette_score(embeddings, tmp_labels)\n",
    "        if score > best_score:\n",
    "            best_score, best_k = score, k\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=best_k)\n",
    "labels = clustering.fit_predict(embeddings)\n",
    "print(f\"Detected ~{best_k} speakers\")\n",
    "\n",
    "\n",
    "# 5️⃣ Frame-to-time mapping\n",
    "frame_duration = signal.shape[1] / fs / embeddings.shape[0]\n",
    "diarization_segments = []\n",
    "current_label = labels[0]\n",
    "start_time = 0.0\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if label != current_label:\n",
    "        end_time = i * frame_duration\n",
    "        diarization_segments.append((start_time, end_time, current_label))\n",
    "        start_time = end_time\n",
    "        current_label = label\n",
    "diarization_segments.append((start_time, len(labels) * frame_duration, current_label))\n",
    "\n",
    "# 6️⃣ Load Faster-Whisper\n",
    "whisper_model = WhisperModel(\n",
    "    \"./models/large-v3\",\n",
    "    device=\"cuda\",\n",
    "    compute_type=\"float16\",\n",
    "    download_root=\"./models/whisper\",\n",
    ")\n",
    "batched_model = BatchedInferencePipeline(model=whisper_model)\n",
    "\n",
    "# 7️⃣ Transcribe audio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5490e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, info = batched_model.transcribe(audio_path, batch_size=16, language='en')  # adjust batch_size if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7139a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 4: Mr. Patil, after reading your reports, I can see that your fatty liver and sugar levels are high. But, don't worry. It's the early stage. Take Metformin 500mg in the morning and evening after eating one tablet. And take 2 tsp of live 1252 syrup twice a day. Stop eating oily and sugary foods.\n",
      "Speaker 1: Walk for 30 minutes. One more thing, do Ultrasound of Abdomen on next visit. I want to see your liver condition, what is the condition. And take medicine continuously for 30 days and follow up. And yes, take food on time. Don't eat late at night. Otherwise, sugar control won't happen. Okay? Let's see.\n"
     ]
    }
   ],
   "source": [
    "# Process each transcription segment\n",
    "for trans_segment in segments:\n",
    "    start = trans_segment.start\n",
    "    end = trans_segment.end\n",
    "    \n",
    "    # Find the speaker segment that overlaps the most with this transcription\n",
    "    max_overlap = 0\n",
    "    best_speaker = None\n",
    "    \n",
    "    # Use the diarization segments (stored earlier) for speaker labels\n",
    "    for spk_segment in diarization_segments:  # This should be the list created earlier with (start, end, label) tuples\n",
    "        spk_start, spk_end, spk_label = spk_segment  # Now this unpacking will work\n",
    "        \n",
    "        # Calculate overlap\n",
    "        overlap_start = max(start, spk_start)\n",
    "        overlap_end = min(end, spk_end)\n",
    "        \n",
    "        if overlap_end > overlap_start:\n",
    "            overlap_duration = overlap_end - overlap_start\n",
    "            if overlap_duration > max_overlap:\n",
    "                max_overlap = overlap_duration\n",
    "                best_speaker = spk_label\n",
    "    \n",
    "    if best_speaker is not None:\n",
    "        print(f\"Speaker {best_speaker + 1}: {trans_segment.text.strip()}\")\n",
    "    else:\n",
    "        print(f\"Unknown Speaker: {trans_segment.text.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebe77f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA failed with error out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 277\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28mprint\u001b[39m(result_text)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m classifier = EncoderClassifier.from_hparams(source=MODEL_DIR, savedir=\u001b[38;5;28;01mNone\u001b[39;00m, run_opts={\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: DEVICE})\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m whisper_model = \u001b[43mWhisperModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWHISPER_MODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfloat16\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./models/whisper\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m batched_model = BatchedInferencePipeline(model=whisper_model)\n\u001b[32m    147\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModels loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Shiva\\miniconda3\\envs\\medd\\Lib\\site-packages\\faster_whisper\\transcribe.py:663\u001b[39m, in \u001b[36mWhisperModel.__init__\u001b[39m\u001b[34m(self, model_size_or_path, device, device_index, compute_type, cpu_threads, num_workers, download_root, local_files_only, files, revision, use_auth_token, **model_kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    655\u001b[39m     model_path = download_model(\n\u001b[32m    656\u001b[39m         model_size_or_path,\n\u001b[32m    657\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   (...)\u001b[39m\u001b[32m    660\u001b[39m         use_auth_token=use_auth_token,\n\u001b[32m    661\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mctranslate2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWhisper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintra_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43minter_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    674\u001b[39m tokenizer_file = os.path.join(model_path, \u001b[33m\"\u001b[39m\u001b[33mtokenizer.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_bytes:\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA failed with error out of memory"
     ]
    }
   ],
   "source": [
    "# robust_diarize_and_label.py\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.signal import medfilt\n",
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "from math import ceil\n",
    "\n",
    "# -------- CONFIG --------\n",
    "MODEL_DIR = \"pretrained_models/spkrec-ecapa-voxceleb\"   # local ECAPA folder\n",
    "AUDIO_PATH = r\"E:\\Projects\\Med_Scribe\\Testing\\audio.wav\"  # local audio (wav or m4a if ffmpeg backend)\n",
    "WHISPER_MODEL_PATH = \"./models/large-v3\"                 # your faster-whisper model folder\n",
    "DEVICE = \"cuda\"                                         # or \"cpu\"\n",
    "WINDOW_SEC = 30.0        # longer context to reduce pitch-sensitivity\n",
    "OVERLAP_SEC = 4.0       # 50% overlap\n",
    "MAX_SPEAKERS = 5\n",
    "SIM_THRESHOLD = 0.80   # cosine similarity threshold for reassignment\n",
    "MEDIAN_KERNEL_SEC = 3.0 # smoothing kernel in seconds (odd integer multiple of frames)\n",
    "# ------------------------\n",
    "\n",
    "def load_audio(path):\n",
    "    sig, sr = torchaudio.load(path)\n",
    "    # ensure mono\n",
    "    if sig.shape[0] > 1:\n",
    "        sig = sig.mean(dim=0, keepdim=True)\n",
    "    return sig, sr\n",
    "\n",
    "def chunk_audio(signal, sr, window_sec, overlap_sec):\n",
    "    win = int(window_sec * sr)\n",
    "    stride = int((window_sec - overlap_sec) * sr)\n",
    "    total = signal.shape[1]\n",
    "    starts = list(range(0, max(1, total - win + 1), stride))\n",
    "    if starts[-1] + win < total:\n",
    "        starts.append(max(0, total - win))\n",
    "    chunks = []\n",
    "    times = []\n",
    "    for s in starts:\n",
    "        e = min(s + win, total)\n",
    "        chunks.append(signal[:, s:e])\n",
    "        times.append((s / sr, e / sr))\n",
    "    return chunks, times\n",
    "\n",
    "def get_embeddings(classifier, chunks):\n",
    "    embs = []\n",
    "    for chunk in chunks:\n",
    "        # classifier.encode_batch accepts waveform like (batch, time) or (channels, time) as you used earlier\n",
    "        with torch.no_grad():\n",
    "            emb = classifier.encode_batch(chunk)  # returns tensor-like\n",
    "        # normalize & flatten to vector\n",
    "        emb = torch.tensor(emb).squeeze().cpu().numpy()\n",
    "        emb = emb.reshape(-1)  # ensure 1D\n",
    "        emb = emb / (np.linalg.norm(emb) + 1e-8)\n",
    "        embs.append(emb)\n",
    "    return np.vstack(embs)  # shape: (n_chunks, emb_dim)\n",
    "\n",
    "def estimate_num_speakers(embeddings, max_k=5):\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "    best_k, best_score = 2, -1\n",
    "\n",
    "    n_samples = len(embeddings)\n",
    "    max_k = min(max_k, n_samples - 1)  # ✅ prevent invalid k\n",
    "\n",
    "    for k in range(2, max_k + 1):\n",
    "        labels = AgglomerativeClustering(n_clusters=k).fit_predict(embeddings)\n",
    "        score = silhouette_score(embeddings, labels)\n",
    "        if score > best_score:\n",
    "            best_k, best_score = k, score\n",
    "    return best_k\n",
    "\n",
    "def cluster_and_reassign(embeddings, sim_threshold, max_k):\n",
    "    # estimate K\n",
    "    k = estimate_num_speakers(embeddings, max_k)\n",
    "    if k == 1:\n",
    "        labels = np.zeros(len(embeddings), dtype=int)\n",
    "    else:\n",
    "        labels = AgglomerativeClustering(n_clusters=k).fit_predict(embeddings)\n",
    "\n",
    "    # compute means\n",
    "    unique = sorted(set(labels))\n",
    "    means = {u: embeddings[labels == u].mean(axis=0) for u in unique}\n",
    "    for u in means:\n",
    "        means[u] = means[u] / (np.linalg.norm(means[u]) + 1e-8)\n",
    "\n",
    "    # reassignment by cosine similarity to means (stability hack)\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        sims = {u: float(np.dot(emb, means[u])) for u in unique}\n",
    "        best_u, best_sim = max(sims.items(), key=lambda x: x[1])\n",
    "        if best_sim >= sim_threshold:\n",
    "            labels[i] = best_u\n",
    "        # else keep original cluster (rare)\n",
    "    return labels, k\n",
    "\n",
    "def labels_to_segments(labels, times):\n",
    "    segs = []\n",
    "    cur_label = labels[0]\n",
    "    cur_start = times[0][0]\n",
    "    for i in range(1, len(labels)):\n",
    "        if labels[i] != cur_label:\n",
    "            cur_end = times[i][0]  # end is start of current chunk\n",
    "            segs.append((cur_start, cur_end, cur_label))\n",
    "            cur_label = labels[i]\n",
    "            cur_start = times[i][0]\n",
    "    # finish\n",
    "    segs.append((cur_start, times[-1][1], cur_label))\n",
    "    return segs\n",
    "\n",
    "def median_smooth_labels(labels, times, kernel_sec):\n",
    "    # map labels to per-chunk sequence and apply medfilt with odd kernel\n",
    "    if kernel_sec <= 0:\n",
    "        return labels\n",
    "    avg_chunk_dur = (times[0][1] - times[0][0])\n",
    "    kernel = int(round(kernel_sec / avg_chunk_dur))\n",
    "    if kernel % 2 == 0:\n",
    "        kernel += 1\n",
    "    if kernel < 1:\n",
    "        kernel = 1\n",
    "    if kernel == 1:\n",
    "        return labels\n",
    "    return medfilt(labels.astype(float), kernel_size=kernel).astype(int)\n",
    "\n",
    "def align_transcript_with_speakers(trans_segments, diarization_segments):\n",
    "    # diarization_segments: list of (start,end,label)\n",
    "    out = []\n",
    "    for t in trans_segments:\n",
    "        s, e, text = t.start, t.end, t.text.strip()\n",
    "        # find diarization segment with max overlap\n",
    "        best_label, best_overlap = None, 0.0\n",
    "        for ds in diarization_segments:\n",
    "            ds_s, ds_e, ds_label = ds\n",
    "            overlap = max(0.0, min(e, ds_e) - max(s, ds_s))\n",
    "            if overlap > best_overlap:\n",
    "                best_overlap = overlap\n",
    "                best_label = ds_label\n",
    "        out.append((best_label, s, e, text))\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    print(\"Loading models...\")\n",
    "    classifier = EncoderClassifier.from_hparams(source=MODEL_DIR, savedir=None, run_opts={\"device\": DEVICE})\n",
    "    whisper_model = WhisperModel(WHISPER_MODEL_PATH, device=DEVICE, compute_type=\"float16\", download_root=\"./models/whisper\")\n",
    "    batched_model = BatchedInferencePipeline(model=whisper_model)\n",
    "    print(\"Models loaded.\")\n",
    "\n",
    "    signal, sr = load_audio(AUDIO_PATH)\n",
    "    print(f\"Audio loaded: {AUDIO_PATH} sr={sr} duration={signal.shape[1]/sr:.2f}s\")\n",
    "\n",
    "    # chunk audio\n",
    "    chunks, times = chunk_audio(signal, sr, WINDOW_SEC, OVERLAP_SEC)\n",
    "    print(f\"{len(chunks)} chunks, window={WINDOW_SEC}s overlap={OVERLAP_SEC}s\")\n",
    "\n",
    "    # embeddings\n",
    "    embeddings = get_embeddings(classifier, chunks)\n",
    "    print(\"Embeddings computed:\", embeddings.shape)\n",
    "\n",
    "    # clustering + reassignment\n",
    "    labels, detected_k = cluster_and_reassign(embeddings, SIM_THRESHOLD, MAX_SPEAKERS)\n",
    "    print(f\"Detected speakers (initial/after): {detected_k}, labels unique: {sorted(set(labels))}\")\n",
    "\n",
    "    # smoothing\n",
    "    labels_sm = median_smooth_labels(labels, times, MEDIAN_KERNEL_SEC)\n",
    "    diarization_segments = labels_to_segments(labels_sm, times)\n",
    "    print(\"Diarization segments (merged):\")\n",
    "    for s,e,l in diarization_segments:\n",
    "        print(f\"  {s:.2f}s - {e:.2f}s : Speaker {l+1}\")\n",
    "\n",
    "    # Transcribe with faster-whisper BatchedInferencePipeline\n",
    "    print(\"Transcribing with faster-whisper...\")\n",
    "    segments_gen, info = batched_model.transcribe(AUDIO_PATH, batch_size=16, language='en')\n",
    "    segments = list(segments_gen)\n",
    "    print(f\"Transcribed {len(segments)} segments\")\n",
    "    transcript_chunks = [segment.text.strip() for segment in segments if segment.text.strip()]\n",
    "    full_transcript = \" \".join(transcript_chunks)\n",
    "    # Align transcript sentences with diarization segments\n",
    "    labeled = align_transcript_with_speakers(segments, diarization_segments)\n",
    "    print(\"\\n===== SPEAKER-LABELED TRANSCRIPT =====\\n\")\n",
    "    for label, s, e, text in labeled:\n",
    "        if label is None:\n",
    "            print(f\"Unknown Speaker: {text}\")\n",
    "        else:\n",
    "            print(f\"Speaker {label+1}: {text}\")\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Entity extraction using Gemma\n",
    "    # -----------------------------\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "    import torch\n",
    "\n",
    "    # Configure 4-bit quantization exactly as before\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "\n",
    "    model_dir = \"./models/gemma-3-1b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"Gemma loaded\")\n",
    "\n",
    "    # Stopping criteria for your AAA delimiter\n",
    "    class StopOnBackticks(StoppingCriteria):\n",
    "        def __init__(self, tokenizer, stop_sequence=\"AAA\"):\n",
    "            self.tokenizer = tokenizer\n",
    "            self.stop_sequence = stop_sequence\n",
    "            self.stop_ids = tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
    "\n",
    "        def __call__(self, input_ids, scores, **kwargs):\n",
    "            if len(input_ids[0]) >= len(self.stop_ids):\n",
    "                if (input_ids[0][-len(self.stop_ids):] == torch.tensor(self.stop_ids, device=input_ids.device)).all():\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "    stopping_criteria = StoppingCriteriaList([StopOnBackticks(tokenizer)])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prepare system + user prompt\n",
    "    # -----------------------------\n",
    "    system_prompt = \"\"\"\n",
    "    You are a medical prescription parser. Extract ONLY information explicitly stated.\n",
    "\n",
    "    Rules:\n",
    "    1. Extract medicines with EXACT dosages mentioned\n",
    "    2. If dosage/frequency unclear, mark as \"unspecified\"\n",
    "    3. Do NOT infer or assume any information\n",
    "    4. If doctor says \"continue previous meds\", extract NOTHING\n",
    "    5. Output only one valid JSON object and stop\n",
    "    6. At the end of the Output print AAA\n",
    "\n",
    "    Output format:\n",
    "    {\n",
    "    \"medicines\": [{\"name\": str, \"dosage\": str, \"frequency\": str, \"duration\": str}],\n",
    "    \"diseases\": [str],\n",
    "    \"tests\": [{\"name\": str, \"timing\": str}]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    {system_prompt}\n",
    "    Extract from this prescription conversation:\n",
    "    {full_transcript}\n",
    "\n",
    "    Remember: Only extract explicitly stated information. No assumptions.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Run Gemma model\n",
    "    # -----------------------------\n",
    "    inputs = tokenizer(user_prompt, return_tensors='pt').to(gemma_model.device)\n",
    "    outputs = gemma_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.01,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(result_text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8aacef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Entity extraction using Gemma\n",
    "# -----------------------------\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization exactly as before\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_dir = \"./models/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "print(\"Gemma loaded\")\n",
    "\n",
    "# Stopping criteria for your AAA delimiter\n",
    "class StopOnBackticks(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_sequence=\"AAA\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_sequence = stop_sequence\n",
    "        self.stop_ids = tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        if len(input_ids[0]) >= len(self.stop_ids):\n",
    "            if (input_ids[0][-len(self.stop_ids):] == torch.tensor(self.stop_ids, device=input_ids.device)).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnBackticks(tokenizer)])\n",
    "\n",
    "# -----------------------------\n",
    "# Prepare system + user prompt\n",
    "# -----------------------------\n",
    "system_prompt = \"\"\"\n",
    "You are a medical prescription parser. Extract ONLY information explicitly stated.\n",
    "\n",
    "Rules:\n",
    "1. Extract medicines with EXACT dosages mentioned\n",
    "2. If dosage/frequency unclear, mark as \"unspecified\"\n",
    "3. Do NOT infer or assume any information\n",
    "4. If doctor says \"continue previous meds\", extract NOTHING\n",
    "5. Output only one valid JSON object and stop\n",
    "6. At the end of the Output print AAA\n",
    "\n",
    "Output format:\n",
    "{\n",
    "  \"medicines\": [{\"name\": str, \"dosage\": str, \"frequency\": str, \"duration\": str}],\n",
    "  \"diseases\": [str],\n",
    "  \"tests\": [{\"name\": str, \"timing\": str}]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "{system_prompt}\n",
    "Extract from this prescription conversation:\n",
    "{full_transcript}\n",
    "\n",
    "Remember: Only extract explicitly stated information. No assumptions.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Run Gemma model\n",
    "# -----------------------------\n",
    "inputs = tokenizer(user_prompt, return_tensors='pt').to(gemma_model.device)\n",
    "outputs = gemma_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.01,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    stopping_criteria=stopping_criteria,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "result_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(result_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
